
[{"content":"‚ÄúLearning is the only thing the mind never exhausts, never fears, and never regrets.‚Äù - Leonardo da Vinci\n","date":"29 October 2024","externalUrl":null,"permalink":"/","section":"Khoi's Blog","summary":"","title":"Khoi's Blog","type":"page"},{"content":" Bach-Khoi Vo # Location: HCM City, Vietnam\nEmail: itskoiwork@gmail.com\nWebsite: itskoi.github.io\nLinkedIn: bachkhoivo\nProfessional Summary # AI Engineer with hands-on experience in building production-grade LLM applications, specializing in Retrieval-Augmented Generation (RAG) architectures and MLOps practices. Published researcher in NLP and multi-modal AI, with expertise in Vietnamese language models. Proven track record in designing scalable AI infrastructure and implementing end-to-end machine learning pipelines. Strong focus on building reliable, production-ready AI systems while maintaining active contributions to open-source LLM tools and technical documentation.\nEducation # University of Science\nBachelor of Science in Computer Science\nAug 2019 ‚Äì Nov 2023\nGPA: 3.74/4.0\nAwards:\nOutstanding Freshman Scholarship, 2019 Encouragement Scholarship for academic years 2019-2022 Recognized for Outstanding Research Activities, 2022-2023 Top 12 in the HCM AI Challenge 2022 (Video Retrieval) Certifications # Google Cloud Skills Boost, Member - Diamond League, 2022 - Present TOEIC 900, IIG Vietnam, Jun 2023 IBM AI Engineer Certificate, Coursera, Jun 2022 - Sep 2022 IBM Data Science Professional Certificate, Coursera, May 2021 - Jun 2021 Publications # Combining Diffusion Model and PhoBERT for Vietnamese Text-to-Image Generation\nIEEE-RIVF‚Äô23, Dec 2023\nAuthors: Bach-Khoi Vo, Anh-Dung Ho, An-Vinh Luong, Dinh Dien\nDOI: 10.1109/RIVF60135.2023.10471860\nSentiment Analysis for Vietnamese Language Using PhoBERT Model\nFAIR‚Äô22, Dec 2022\nAuthors: Thanh-Tu Huynh, Bach-Khoi Vo, Anh-Dung Ho, Duc-Lung Vu\nDOI: 10.15625/vap.2022.0254\nExperience # AI Engineer\nUniquify, Oct 2023 ‚Äì Present\nLed development of an LLM-based chatbot for SoC documentation, implementing RAG and MLOps for reliability and scalability. Developed AI training modules covering CV, NLP, LLMs, and MLOps to support team knowledge building. Deployed a real-time ETL pipeline using Kafka and Schema Registry for structured data processing. Conducted seminars and workshops, producing documentation for continuous learning and effective knowledge sharing. Research Assistant\nComputational Linguistics Center (CLC) - HCMUS, Apr 2022 - Jun 2023\nContributed to Vietnamese text-to-image synthesis model research, combining Diffusion Models and PhoBERT, achieving state-of-the-art results. Improved PhoBERT architecture for sentiment analysis on Vietnamese, resulting in a 95.22% F1-score on the UIT-VSFC dataset. AI Engineer Intern\nITR, Feb 2023 - Apr 2023\nLed a team for an R\u0026amp;D project on automatic spelling correction in medical reports using deep learning for the healthcare sector. Managed task assignments, milestones, and ensured timely project completion. Personal Projects # arXivRAG\nGitHub Repository, Sep 2024 - Present\nAn open-source tool for retrieving and summarizing academic content from arXiv, using Docker, Milvus, MinIO, LlamaIndex, Ollama/Hugging Face, FastAPI, MongoDB/Redis, Chainlit/Gradio, Nginx, Prometheus, Grafana, and the EFK Stack.\nPersonal Blog\nitskoi.github.io, Oct 2024 - Present\nTechnical blog documenting AI concepts, including IR, LLM, MLOps, and Software \u0026amp; Data Engineering practices.\nTechnologies # LLM \u0026amp; NLP # Development: LangChain, LlamaIndex, Hugging Face Transformers, NLTK, spaCy LLM Serving: Ollama, vLLM Vector Databases: Milvus, Qdrant, Chroma MLOps \u0026amp; Infrastructure # Data Stack: RabbitMQ, Kafka, MongoDB, PostgreSQL, MinIO, FastAPI Orchestration \u0026amp; Monitoring: Docker, Airflow, Prometheus, EFK Stack Cloud: Google Cloud Platform (GCP) Data Science # Computer Vision: OpenCV, PIL, Scikit-image, Ultralytics Deep Learning: PyTorch, TensorFlow, Keras Machine Learning: Scikit-learn, XGBoost Data Processing: NumPy, Pandas, SciPy Visualization: Matplotlib, Seaborn, Streamlit Development Tools \u0026amp; Environment # Programming: Python, SQL, Bash Version Control: Git OS: Linux, macOS, Windows ","date":"29 October 2024","externalUrl":null,"permalink":"/resume/","section":"Khoi's Blog","summary":"","title":"Resume","type":"page"},{"content":"","date":"29 October 2024","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","date":"15 October 2024","externalUrl":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts","type":"posts"},{"content":"","date":"15 October 2024","externalUrl":null,"permalink":"/categories/text-retrieval/","section":"Categories","summary":"","title":"Text Retrieval","type":"categories"},{"content":" Information Retrieval (IR) is a well-established field, and text retrieval is one of its core tasks. With the growing applications of IR in the digital era, efficient text retrieval has become essential. The rise of Retrieval-Augmented Generation (RAG) has heightened the demand for fast, accurate retrievers.\nIn developing RAG systems, this series of posts serves as my notes and reflections on building effective system. Let\u0026rsquo;s delve into text retrieval!\nNote: These are my personal learning notes, so some points may not be entirely accurate. I strive to improve my understanding and will correct any errors I find. If you spot any inaccuracies, please feel free to share your insights to help enhance the content üòä.\nObjective # In this post, I‚Äôll cover the fundamental terms and concepts you\u0026rsquo;ll encounter in text retrieval, giving a big-picture overview rather than fine-grained details. This will help you understand the essential ideas and terminology needed to explore the field further.\nFormulate the problem # Given a set of queries q and documents d, the task is to identify the most relevant document for each query based on the \u0026ldquo;similarity score\u0026rdquo; between them.\nTo tackle this, we need to address two key questions:\nHow should we represent the text? Based on these representations, how can we calculate a meaningful similarity score between queries and documents? A basic approach to text retrieval is word matching or term matching. Initially, we can represent text using a Bag-of-Words (BoW) model, where each word occupies a dimension in a sparse, high-dimensional vector space. However, this high-dimensional approach can be computationally intensive. To make retrieval more efficient, we can transform the text into lower-dimensional dense embeddings‚Äîcompact representations that retain semantic meaning.\nOnce we have these embeddings, our task is to calculate similarity scores between these vectors effectively.\nLet‚Äôs explore each part in more detail.\nText Representation # Word matching # The naive approach is to match words between a query and a document. Using a basic algorithm, we start with a similarity score of zero and increase it each time a matching word is found between query and document.\nThis approach has a time complexity of \\(O(|q| * |d| * l_q * l_d)\\) on average, where |q| and |d| are the number of queries and documents, respectively, and \\(l_q\\), \\(l_d\\) are the average words in a query and document. By pre-processing documents into sets of words, we can reduce lookup operations to \\(O(1)\\) on average, decreasing time complexity to \\(O(|q| * |d| * l_q)\\). For long documents, this improvement significantly reduces runtime.\nSparse Embeddings # Building on word matching, we can create one-hot vector embeddings for each document with dimensions equal to the vocabulary size, \\(|v|\\). This Bag-of-Words representation, or sparse embedding, contains mostly zeros since only a small fraction of the vocabulary appears in any single text.\nFor each query, we create a one-hot vector and compute similarity scores across documents. Although this method has similar time complexity to word matching, it allows for parallel computing techniques like matrix multiplication on GPU/TPU, often used in machine learning to improve efficiency.\n\u0026#x1f4a1;Key insight: Embedding text information into vectors enables efficient computation of similarity scores.\nBy introducing weighting (e.g., using TF-IDF or BM25), we assign greater significance to key terms, enhancing retrieval quality while retaining simplicity.\nSparse embeddings, however, have limitations, especially in distinguishing synonyms, handling ambiguous terms, and still lack semantic awareness, pushing us toward dense embeddings.\nDense Embeddings # Dense embeddings focus on capturing semantic meaning by encoding text into low-dimensional vectors. Two common approaches are:\nWord embeddings: Combine embeddings for each word in the text (e.g., Word2Vec, GloVe, etc). Sentence embeddings: Represent entire text passages, capturing not only word meanings but also relationships between them, typically using transformer-based models. Using dense embeddings also addresses the problem of high-dimensional, sparse vectors in Bag-of-Words, reducing the vector space while retaining richer semantic context.\nWord embedding Common approaches for word embedding include Word2Vec, GloVe,etc. In these methods, embeddings are generated for individual words and then combined using linear projections or similar techniques. However, because information is extracted independently from each word and then fused with a general merging approach, the final text embedding often lacks a coherent relationship between words, resulting in suboptimal representations.\nSentence embedding Unlike word embeddings, sentence embeddings capture not only the meaning of individual words but also the relationships between them within a sentence. This holistic approach provides better performance compared to combining individual word embeddings. Transformer-based models are currently the most popular choice for generating these embeddings, as they excel at capturing semantic and syntactic relationships within the text.\nInitially, a single encoder might be used to embed both queries and documents without distinction. However, this poses two main issues:\nStructural Mismatch: Queries and documents often differ in structure, with queries, for example, frequently being Wh-questions. Lack of Relevance Modeling: When each document and query is embedded independently, the context and meaning are isolated, lacking a shared relevance between them. Let‚Äôs explore how to address these issues.\nBi-encoder or Two-tower # To better capture structural differences between queries and documents, we can use a bi-encoder or dual-encoder or two-tower approach, where two independent encoders process queries and documents. For instance, BERT can serve as the backbone for both encoders, fine-tuned using contrastive learning. This setup significantly improves retrieval performance compared to using a single encoder for all input.\nAfter having embeddings for both query and document, we need to compute the similarity between them. In common, we have 3 methods:\nL2 Distance (Euclidean Distance): measures the straight-line distance between two vectors in a multidimensional space. Cosine Similarity: measures the cosine of the angle between two vectors, which reflects their orientation rather than their magnitude. Dot Product: represents the magnitude of alignment between two vectors in terms of both their magnitudes and the cosine of the angle between them. Use whatever you like if it differentiate if the 2 vectors is not similar and emphasize if 2 vector\nCross-encoder # To address the lack of semantic interaction in bi-encoders, cross-encoders combine both the query and document into a single input separated by a special token (e.g., query\u0026lt;sep\u0026gt;document).\nThis setup allows the model to learn complex relationships between queries and documents, often leading to more accurate retrieval. However, the trade-off is speed, making cross-encoders less practical for large document collections.\nAs a result, cross-encoders are often used as rerankers in two-stage retrieval systems, with bi-encoders serving as the initial retrievers.\nConclusion # In this post, we introduced the text retrieval problem and covered techniques ranging from simple text matching to sophisticated cross-encoder models for capturing query-document relationships.\nIn future posts, we‚Äôll dive into the bi-encoder, cross-encoder, two-stage retrieval paradigm, examining the components, challenges, and optimizations in detail. Stay tuned! üòä\n","date":"15 October 2024","externalUrl":null,"permalink":"/posts/text-retrieval-series-%231-fundamentals-and-the-big-picture/","section":"Posts","summary":"","title":"Text Retrieval Series #1: Fundamentals and the Big Picture","type":"posts"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"},{"content":"","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"}]