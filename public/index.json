
[{"content":"‚ÄúLearning is the only thing the mind never exhausts, never fears, and never regrets.‚Äù - Leonardo da Vinci\n","date":"31 October 2024","externalUrl":null,"permalink":"/","section":"Khoi's Blog","summary":"","title":"Khoi's Blog","type":"page"},{"content":"","date":"31 October 2024","externalUrl":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts","type":"posts"},{"content":" TL;DR: ViperGPT proposes a simple framework for solving complex visual queries by integrating code-generation models into vision with an API and the Python interpreter.\nNote: This my personal learning note, so some points may not be entirely accurate. I strive to improve my understanding and will correct any errors I find. If you spot any inaccuracies, please feel free to share your insights to help enhance the content üòä.\nSummary # The framework\u0026rsquo;s ability to generate and execute Python code, coupled with its access to a diverse set of pre-trained vision and language models through a well-defined API, enables ViperGPT to achieve impressive zero-shot performance on a wide range of visual reasoning tasks.\nMotivation # ViperGPT is motivated by the limitations of end-to-end models, which are the dominant approach for answering visual queries. While end-to-end models have made significant progress in individual tasks such as object recognition and depth estimation, they struggle with more complex tasks that require both visual processing and reasoning. ViperGPT seeks to overcome these limitations by introducing a modular and interpretable approach to visual reasoning.\nBefore ViperGPT # Lack of Compositional Reasoning: End-to-end models do not explicitly differentiate between visual processing and reasoning, making it difficult for them to handle complex, multi-step tasks. People naturally decompose visual tasks into smaller steps, but end-to-end models lack this inherent compositional reasoning ability. Limited Interpretability: End-to-end models offer little insight into their decision-making process, making it difficult to understand why they succeed or fail. This lack of interpretability, described as having no way to audit the result of each step to diagnose failure, becomes increasingly problematic as models become more complex and data-hungry. Inability to Leverage Existing Models and Tools: End-to-end models necessitate training a new model for every new task, failing to capitalize on the advancements in fundamental vision tasks and readily available tools like mathematical operations. Ideally, new tasks should be tackled by recombining existing models and tools in innovative ways without requiring additional training3. ViperGPT # Problem formulation # Given a visual input x and a textual query q about its contents, it first synthesizes a program z = œÄ(q) with a program generator œÄ given the query. It then applies the execution engine r = œï(x, z) to execute the program z on the input x and produce a result r. This framework is flexible, supporting image or videos as inputs x, questions or descriptions as queries q, and any type (e.g., text or image crops) as outputs r.\nProgram generator # This process uses a large language model (LLM) called Codex to create Python programs that can answer a given query using various vision and language modules. The LLM is provided with an API specifying the available modules and their functionalities1. This process requires no training for specific tasks\nAPI Specification # The API specification plays a crucial role in ViperGPT by serving as a bridge between the code-generating LLM (Codex) and the various vision and language modules that it uses to answer visual queries. Instead of directly implementing the API, the authors of ViperGPT made the design decision to provide Codex with a specification of the API, including function signatures, docstrings, and examples.\nProgram Executor # The generated Python programs are then executed by the Python interpreter, which utilizes pre-trained models to carry out the instructions within the program. This execution engine allows the model to perform logical operations through Python and perceptual tasks through pre-trained models. This process ensures the program\u0026rsquo;s interpretability as each step is explicitly outlined with code function calls and intermediate values that can be examined.\nWhy ViperGPT stands out? # Interpretability: Every step of the reasoning process is explicit and can be inspected, as it is represented in the form of code. Compositionality: Tasks are decomposed into smaller sub-tasks, promoting a more human-like approach to visual reasoning. Logical Reasoning: ViperGPT leverages built-in Python logical and mathematical operators, enabling clear and explicit logical reasoning. Flexibility: ViperGPT seamlessly incorporates any vision or language module by simply adding its specification to the API. Adaptability: Improvements in individual modules directly enhance ViperGPT\u0026rsquo;s performance. Training-free: ViperGPT eliminates the need for retraining or fine-tuning for every new task. Generality: A single system unifies various visual tasks. Conclusion # ViperGPT presents a compelling alternative to end-to-end models for complex visual tasks. By leveraging the power of programmatic composition and pre-trained modules, it achieves strong zero-shot performance while maintaining interpretability and flexibility. As the field of vision and language models continues to advance, ViperGPT\u0026rsquo;s ability to readily incorporate these advancements makes it a promising direction for future research.\nReference # https://viper.cs.columbia.edu/ ","date":"31 October 2024","externalUrl":null,"permalink":"/posts/reading-papers-vipergpt/","section":"Posts","summary":"","title":"Reading Papers: ViperGPT","type":"posts"},{"content":"","date":"30 October 2024","externalUrl":null,"permalink":"/categories/reading-papers/","section":"Categories","summary":"","title":"Reading Papers","type":"categories"},{"content":" TL;DR: ColBERT combines the efficiency of a Bi-encoder, allowing fast embedding and precomputing, with a late interaction mechanism to learn cross-information between query and document. The result? Near-BERT performance at a fraction of BERT‚Äôs runtime!\nNote: This my personal learning note, so some points may not be entirely accurate. I strive to improve my understanding and will correct any errors I find. If you spot any inaccuracies, please feel free to share your insights to help enhance the content üòä.\nSummary # ColBERT leverages independent encodings for queries and documents to generate fine-grained embeddings. It then applies a lightweight interaction mechanism to compute relevance scores. This innovative approach retains the expressiveness of BERT while significantly accelerating query processing. ColBERT, therefore, achieves comparable results to BERT-based models while drastically reducing computational overhead.\nMotivation # ColBERT was created to address the trade-off between quality and cost in neural IR models, particularly for deep language models like BERT. While BERT has shown state-of-the-art effectiveness in passage search tasks, it is computationally expensive, making it difficult to deploy in real-world scenarios where low latency is crucial.\nBefore ColBERT # Prior to ColBERT, ranking models could be grouped into three main paradigms:\nNo Interaction: Queries and documents have separate, independent encoders that capture the meaning of their respective inputs. However, embedding each query and document independently results in isolated representations, with no cross-information to link the two. This is known as the bi-encoder or two-tower paradigm. Partial Interaction: This approach attempts to capture relationships between the query and document but lacks the internal self-information of each input, effectively acting as an \u0026ldquo;inverted\u0026rdquo; version of the no-interaction paradigm. Full Interaction: Transformer-based models like BERT capture both self and cross information between the query and document. Although BERT-based models offer strong performance, their high computational costs pose challenges, especially in latency-sensitive applications. ColBERT‚Äôs Novel Approach # ColBERT bridges the gap between effectiveness and efficiency in models that leverage deep language models like BERT.\nColBERT introduces a \u0026ldquo;late interaction\u0026rdquo; paradigm, where queries and documents are independently encoded into sets of contextualized embeddings using BERT. A lightweight interaction mechanism, the MaxSim operation, then models fine-grained similarity between the query and document representations.\nUnderstanding MaxSim operation # For each word vector embedding in the query bag embeddings \\(E_q\\), ColBERT finds the maximum word similarity score with the word embeddings in the document \\(E_d\\). This is done through the MaxSim operator.\nThe MaxSim operator calculates the cosine similarity between a single query embedding and all embeddings in document\u0026rsquo;s bag embedding. The highest similarity score is then selected as the \u0026ldquo;match\u0026rdquo; for that specific query term. This process is repeated for all embeddings in \\(E_q\\). The individual term scores are then summed to estimate the overall relevance of the document to the query.\nWhy ColBERT stands out? # ColBERT\u0026rsquo;s late interaction architecture for ranking documents in response to search queries relies on the MaxSim operator, which finds the highest cosine similarity score between each query term embedding and all document embeddings. Here‚Äôs why ColBERT is both effective and efficient:\nComputational Efficiency: MaxSim avoids the heavy computational requirements of attention mechanisms: MaxSim involves calculating cosine similarity between query and document embeddings and then selecting the highest score per query term. In contrast, Attention mechanisms require multiple matrix multiplications and normalization steps, which are computationally expensive. Pre-computing document embeddings further accelerates ColBERT, as seen in no-interaction paradigms. Pruning Capability: MaxSim allows vector-similarity search indexes to efficiently identify documents with terms highly similar (top-k) to the query, without scoring every document in the collection. Attention mechanisms, on the other hand, don‚Äôt lend themselves to such pruning. Although simple, MaxSim achieves state-of-the-art results when paired with BERT‚Äôs contextualized embeddings. ColBERT performance # In Table 1 of the referenced article, ColBERT demonstrates competitive performance on ranking tasks compared to BERT, but with a significant reduction in runtime.\nThe Figure 4 highlights ColBERT‚Äôs runtime efficiency: as the number of candidate documents (k) increases, ColBERT‚Äôs runtime remains relatively stable, underscoring its scalability and speed advantage over BERT.\nShort-Answer Quiz # Questions\nWhat is the main challenge that ColBERT addresses in the context of neural ranking models? Describe the \u0026ldquo;late interaction\u0026rdquo; paradigm introduced by ColBERT. How does ColBERT\u0026rsquo;s architecture differ from traditional interaction-focused rankers (e.g., DRMM, KNRM)? Explain the role of query augmentation in ColBERT\u0026rsquo;s query encoder. How does ColBERT\u0026rsquo;s document encoder process document text and generate embeddings? Describe the late interaction mechanism in ColBERT and how it calculates the relevance score. What are the benefits of using a MaxSim-based interaction mechanism in ColBERT? How does ColBERT achieve efficiency in terms of offline indexing and query processing? Explain how ColBERT supports end-to-end retrieval from a large document collection. What are the key findings from the experimental evaluation of ColBERT on MS MARCO and TREC CAR datasets? Answer Key\nColBERT addresses the challenge of high computational cost associated with BERT-based ranking models, aiming to improve their efficiency without sacrificing effectiveness. Late interaction refers to the independent encoding of queries and documents into sets of contextualized embeddings followed by a lightweight interaction step to compute the relevance score. This allows for efficient pre-computation of document representations offline, reducing the online computational burden. Unlike traditional interaction-focused rankers that compute a single relevance score based on pairwise interactions between all query and document terms, ColBERT separates query and document encoding and relies on MaxSim to identify the most relevant document terms for each query term and aggregates these matches. Query augmentation involves padding the query with masked tokens, allowing BERT to generate query-based embeddings at these positions. This mechanism serves as a differentiable way to expand queries with new terms or re-weight existing terms based on their importance for matching. The document encoder segments the document into tokens, prepends special tokens, and passes the sequence through BERT. After applying a linear layer, it filters out embeddings corresponding to punctuation symbols to reduce the number of embeddings per document. ColBERT\u0026rsquo;s late interaction mechanism computes the maximum similarity (cosine or L2 distance) between each query embedding and all document embeddings. The resulting maximum similarity scores are summed across all query terms to obtain the overall relevance score for the document. The MaxSim operation offers efficiency by enabling pruning for top-k retrieval. It allows using vector-similarity search indexes to efficiently retrieve the most promising document candidates without exhaustively scoring all documents. ColBERT pre-computes document representations offline, reducing the online computational load. During query processing, it encodes the query only once and interacts with the pre-computed document representations, further minimizing the cost. ColBERT leverages its pruning-friendly MaxSim operation to support end-to-end retrieval. It employs a vector-similarity search index (e.g., Faiss) to retrieve a subset of candidate documents based on the similarity of individual query embeddings to all document embeddings. This candidate set is then re-ranked using ColBERT\u0026rsquo;s full scoring mechanism. Experimental results on MS MARCO and TREC CAR show that ColBERT achieves competitive effectiveness compared to state-of-the-art BERT-based models while being significantly more efficient. It outperforms other non-BERT baselines and demonstrates a considerable speedup and reduction in FLOPs compared to BERT. Conclusion # ColBERT offers a powerful ==balance between speed and accuracy== in information retrieval by combining the efficiency of a Bi-encoder with a novel late interaction mechanism. Its MaxSim operator allows for fine-grained similarity matching without the high computational costs of full attention, making it ideal for latency-sensitive applications. ColBERT‚Äôs performance, ==close to BERT but at a fraction of the runtime==, demonstrates the potential of innovative interaction mechanisms in neural IR.\nReferences # https://arxiv.org/pdf/2004.12832 https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/ ","date":"30 October 2024","externalUrl":null,"permalink":"/posts/reading-papers-colbertv1/","section":"Posts","summary":"","title":"Reading Papers: ColBERTv1","type":"posts"},{"content":"","date":"30 October 2024","externalUrl":null,"permalink":"/categories/text-retrieval/","section":"Categories","summary":"","title":"Text Retrieval","type":"categories"},{"content":" Bach-Khoi Vo # Location: HCM City, Vietnam\nEmail: itskoiwork@gmail.com\nWebsite: itskoi.github.io\nLinkedIn: bachkhoivo\nProfessional Summary # AI Engineer with hands-on experience in building production-grade LLM applications, specializing in Retrieval-Augmented Generation (RAG) architectures and MLOps practices. Published researcher in NLP and multi-modal AI, with expertise in Vietnamese language models. Proven track record in designing scalable AI infrastructure and implementing end-to-end machine learning pipelines. Strong focus on building reliable, production-ready AI systems while maintaining active contributions to open-source LLM tools and technical documentation.\nEducation # University of Science\nBachelor of Science in Computer Science\nAug 2019 ‚Äì Nov 2023\nGPA: 3.74/4.0\nAwards:\nOutstanding Freshman Scholarship, 2019 Encouragement Scholarship for academic years 2019-2022 Recognized for Outstanding Research Activities, 2022-2023 Top 12 in the HCM AI Challenge 2022 (Video Retrieval) Certifications # Google Cloud Skills Boost, Member - Diamond League, 2022 - Present TOEIC 900, IIG Vietnam, Jun 2023 IBM AI Engineer Certificate, Coursera, Jun 2022 - Sep 2022 IBM Data Science Professional Certificate, Coursera, May 2021 - Jun 2021 Publications # Combining Diffusion Model and PhoBERT for Vietnamese Text-to-Image Generation\nIEEE-RIVF‚Äô23, Dec 2023\nAuthors: Bach-Khoi Vo, Anh-Dung Ho, An-Vinh Luong, Dinh Dien\nDOI: 10.1109/RIVF60135.2023.10471860\nSentiment Analysis for Vietnamese Language Using PhoBERT Model\nFAIR‚Äô22, Dec 2022\nAuthors: Thanh-Tu Huynh, Bach-Khoi Vo, Anh-Dung Ho, Duc-Lung Vu\nDOI: 10.15625/vap.2022.0254\nExperience # AI Engineer\nUniquify, Oct 2023 ‚Äì Present\nLed development of an LLM-based chatbot for SoC documentation, implementing RAG and MLOps for reliability and scalability. Developed AI training modules covering CV, NLP, LLMs, and MLOps to support team knowledge building. Deployed a real-time ETL pipeline using Kafka and Schema Registry for structured data processing. Conducted seminars and workshops, producing documentation for continuous learning and effective knowledge sharing. Research Assistant\nComputational Linguistics Center (CLC) - HCMUS, Apr 2022 - Jun 2023\nContributed to Vietnamese text-to-image synthesis model research, combining Diffusion Models and PhoBERT, achieving state-of-the-art results. Improved PhoBERT architecture for sentiment analysis on Vietnamese, resulting in a 95.22% F1-score on the UIT-VSFC dataset. AI Engineer Intern\nITR, Feb 2023 - Apr 2023\nLed a team for an R\u0026amp;D project on automatic spelling correction in medical reports using deep learning for the healthcare sector. Managed task assignments, milestones, and ensured timely project completion. Personal Projects # arXivRAG\nGitHub Repository, Sep 2024 - Present\nAn open-source tool for retrieving and summarizing academic content from arXiv, using Docker, Milvus, MinIO, LlamaIndex, Ollama/Hugging Face, FastAPI, MongoDB/Redis, Chainlit/Gradio, Nginx, Prometheus, Grafana, and the EFK Stack.\nPersonal Blog\nitskoi.github.io, Oct 2024 - Present\nTechnical blog documenting AI concepts, including IR, LLM, MLOps, and Software \u0026amp; Data Engineering practices.\nTechnologies # LLM \u0026amp; NLP # Development: LangChain, LlamaIndex, Hugging Face Transformers, NLTK, spaCy LLM Serving: Ollama, vLLM Vector Databases: Milvus, Qdrant, Chroma MLOps \u0026amp; Infrastructure # Data Stack: RabbitMQ, Kafka, MongoDB, PostgreSQL, MinIO, FastAPI Orchestration \u0026amp; Monitoring: Docker, Airflow, Prometheus, EFK Stack Cloud: Google Cloud Platform (GCP) Data Science # Computer Vision: OpenCV, PIL, Scikit-image, Ultralytics Deep Learning: PyTorch, TensorFlow, Keras Machine Learning: Scikit-learn, XGBoost Data Processing: NumPy, Pandas, SciPy Visualization: Matplotlib, Seaborn, Streamlit Development Tools \u0026amp; Environment # Programming: Python, SQL, Bash Version Control: Git OS: Linux, macOS, Windows ","date":"29 October 2024","externalUrl":null,"permalink":"/resume/","section":"Khoi's Blog","summary":"","title":"Resume","type":"page"},{"content":"","date":"29 October 2024","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":" Information Retrieval (IR) is a well-established field, and text retrieval is one of its core tasks. With the growing applications of IR in the digital era, efficient text retrieval has become essential. The rise of Retrieval-Augmented Generation (RAG) has heightened the demand for fast, accurate retrievers.\nIn developing RAG systems, this series of posts serves as my notes and reflections on building effective system. Let\u0026rsquo;s delve into text retrieval!\nNote: These are my personal learning notes, so some points may not be entirely accurate. I strive to improve my understanding and will correct any errors I find. If you spot any inaccuracies, please feel free to share your insights to help enhance the content üòä.\nObjective # In this post, I‚Äôll cover the fundamental terms and concepts you\u0026rsquo;ll encounter in text retrieval, giving a big-picture overview rather than fine-grained details. This will help you understand the essential ideas and terminology needed to explore the field further.\nFormulate the problem # Given a set of queries q and documents d, the task is to identify the most relevant document for each query based on the \u0026ldquo;similarity score\u0026rdquo; between them.\nTo tackle this, we need to address two key questions:\nHow should we represent the text? Based on these representations, how can we calculate a meaningful similarity score between queries and documents? A basic approach to text retrieval is word matching or term matching. Initially, we can represent text using a Bag-of-Words (BoW) model, where each word occupies a dimension in a sparse, high-dimensional vector space. However, this high-dimensional approach can be computationally intensive. To make retrieval more efficient, we can transform the text into lower-dimensional dense embeddings‚Äîcompact representations that retain semantic meaning.\nOnce we have these embeddings, our task is to calculate similarity scores between these vectors effectively.\nLet‚Äôs explore each part in more detail.\nText Representation # Word matching # The naive approach is to match words between a query and a document. Using a basic algorithm, we start with a similarity score of zero and increase it each time a matching word is found between query and document.\nThis approach has a time complexity of \\(O(|q| * |d| * l_q * l_d)\\) on average, where |q| and |d| are the number of queries and documents, respectively, and \\(l_q\\), \\(l_d\\) are the average words in a query and document. By pre-processing documents into sets of words, we can reduce lookup operations to \\(O(1)\\) on average, decreasing time complexity to \\(O(|q| * |d| * l_q)\\). For long documents, this improvement significantly reduces runtime.\nSparse Embeddings # Building on word matching, we can create one-hot vector embeddings for each document with dimensions equal to the vocabulary size, \\(|v|\\). This Bag-of-Words representation, or sparse embedding, contains mostly zeros since only a small fraction of the vocabulary appears in any single text.\nFor each query, we create a one-hot vector and compute similarity scores across documents. Although this method has similar time complexity to word matching, it allows for parallel computing techniques like matrix multiplication on GPU/TPU, often used in machine learning to improve efficiency.\n\u0026#x1f4a1;Key insight: Embedding text information into vectors enables efficient computation of similarity scores.\nBy introducing weighting (e.g., using TF-IDF or BM25), we assign greater significance to key terms, enhancing retrieval quality while retaining simplicity.\nSparse embeddings, however, have limitations, especially in distinguishing synonyms, handling ambiguous terms, and still lack semantic awareness, pushing us toward dense embeddings.\nDense Embeddings # Dense embeddings focus on capturing semantic meaning by encoding text into low-dimensional vectors. Two common approaches are:\nWord embeddings: Combine embeddings for each word in the text (e.g., Word2Vec, GloVe, etc). Sentence embeddings: Represent entire text passages, capturing not only word meanings but also relationships between them, typically using transformer-based models. Using dense embeddings also addresses the problem of high-dimensional, sparse vectors in Bag-of-Words, reducing the vector space while retaining richer semantic context.\nWord embedding Common approaches for word embedding include Word2Vec, GloVe,etc. In these methods, embeddings are generated for individual words and then combined using linear projections or similar techniques. However, because information is extracted independently from each word and then fused with a general merging approach, the final text embedding often lacks a coherent relationship between words, resulting in suboptimal representations.\nSentence embedding Unlike word embeddings, sentence embeddings capture not only the meaning of individual words but also the relationships between them within a sentence. This holistic approach provides better performance compared to combining individual word embeddings. Transformer-based models are currently the most popular choice for generating these embeddings, as they excel at capturing semantic and syntactic relationships within the text.\nInitially, a single encoder might be used to embed both queries and documents without distinction. However, this poses two main issues:\nStructural Mismatch: Queries and documents often differ in structure, with queries, for example, frequently being Wh-questions. Lack of Relevance Modeling: When each document and query is embedded independently, the context and meaning are isolated, lacking a shared relevance between them. Let‚Äôs explore how to address these issues.\nBi-encoder or Two-tower # To better capture structural differences between queries and documents, we can use a bi-encoder or dual-encoder or two-tower approach, where two independent encoders process queries and documents. For instance, BERT can serve as the backbone for both encoders, fine-tuned using contrastive learning. This setup significantly improves retrieval performance compared to using a single encoder for all input.\nAfter having embeddings for both query and document, we need to compute the similarity between them. In common, we have 3 methods:\nL2 Distance (Euclidean Distance): measures the straight-line distance between two vectors in a multidimensional space. Cosine Similarity: measures the cosine of the angle between two vectors, which reflects their orientation rather than their magnitude. Dot Product: represents the magnitude of alignment between two vectors in terms of both their magnitudes and the cosine of the angle between them. Use any measure that clearly distinguishes dissimilar vectors while strongly emphasizing when two vectors are identical.\nCross-encoder # To address the lack of semantic interaction in bi-encoders, cross-encoders combine both the query and document into a single input separated by a special token (e.g., query\u0026lt;sep\u0026gt;document).\nThis setup allows the model to learn complex relationships between queries and documents, often leading to more accurate retrieval. However, the trade-off is speed, making cross-encoders less practical for large document collections.\nAs a result, cross-encoders are often used as rerankers in two-stage retrieval systems, with bi-encoders serving as the initial retrievers.\nConclusion # In this post, we introduced the text retrieval problem and covered techniques ranging from simple text matching to sophisticated cross-encoder models for capturing query-document relationships.\nIn future posts, we‚Äôll dive into the bi-encoder, cross-encoder, two-stage retrieval paradigm, examining the components, challenges, and optimizations in detail. Stay tuned! üòä\n","date":"15 October 2024","externalUrl":null,"permalink":"/posts/text-retrieval-series-%231-fundamentals-and-the-big-picture/","section":"Posts","summary":"","title":"Text Retrieval Series #1: Fundamentals and the Big Picture","type":"posts"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"},{"content":"","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"}]